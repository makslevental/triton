#blocked = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 16], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [16, 4], warpsPerCTA = [1, 8], order = [0, 1]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [8], order = [0]}>
#mma = #ttg.amd_mfma<{versionMajor = 4, versionMinor = 0, warpsPerCTA = [8, 1], instrShape = [32, 32], isTransposed = true}>
#shared = #ttg.swizzled_shared<{vec = 8, perPhase = 1, maxPhase = 16, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 8, perPhase = 1, maxPhase = 16, order = [0, 1]}>
#shared2 = #ttg.swizzled_shared<{vec = 32, perPhase = 1, maxPhase = 4, order = [1, 0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 8 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @attn_fwd(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, %arg3: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, %arg4: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}, %arg18: i32 {tt.divisibility = 16 : i32}, %arg19: i32 {tt.divisibility = 16 : i32}, %arg20: i32 {tt.divisibility = 16 : i32}, %arg21: i32 {tt.divisibility = 16 : i32}, %arg22: i32 {tt.divisibility = 16 : i32}, %arg23: f32, %arg24: i32, %arg25: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, %arg26: i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<0.127517432> : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
    %cst_0 = arith.constant dense<0.127517432> : tensor<256x64xf32, #mma>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<256x128xf32, #mma>
    %cst_2 = arith.constant dense<1.000000e+00> : tensor<256x1xf32, #mma>
    %cst_3 = arith.constant dense<1.000000e+00> : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
    %cst_4 = arith.constant dense<0xFF800000> : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
    %c16640_i32 = arith.constant 16640 : i32
    %c786432_i32 = arith.constant 786432 : i32
    %true = arith.constant true
    %c1_i32 = arith.constant 1 : i32
    %c256_i32 = arith.constant 256 : i32
    %c16384_i32 = arith.constant 16384 : i32
    %c64_i32 = arith.constant 64 : i32
    %cst_5 = arith.constant dense<16384> : tensor<256x1xi32, #blocked>
    %c8000_i32 = arith.constant 8000 : i32
    %cst_6 = arith.constant dense<true> : tensor<64x128xi1, #blocked>
    %cst_7 = arith.constant dense<true> : tensor<128x64xi1, #blocked1>
    %c2_i32 = arith.constant 2 : i32
    %cst_8 = arith.constant dense<0.000000e+00> : tensor<256x64xf32, #mma>
    %cst_9 = arith.constant dense<true> : tensor<256x128xi1, #mma>
    %cst_10 = arith.constant dense<16384> : tensor<256x1xi32, #mma>
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    llvm.intr.assume %true : i1
    %0 = tt.get_program_id x : i32
    %1 = tt.get_program_id y : i32
    %2 = tt.get_program_id z : i32
    %3 = arith.muli %0, %c256_i32 : i32
    %4 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #mma}>>
    %5 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>
    %6 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked2>
    %7 = tt.splat %3 : i32 -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #mma}>>
    %8 = tt.splat %3 : i32 -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>
    %9 = arith.addi %7, %4 : tensor<256xi32, #ttg.slice<{dim = 1, parent = #mma}>>
    %10 = arith.addi %8, %5 : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>
    %11 = arith.muli %2, %arg5 : i32
    %12 = tt.addptr %arg0, %11 : !tt.ptr<f16>, i32
    %13 = arith.muli %1, %arg6 : i32
    %14 = tt.addptr %12, %13 : !tt.ptr<f16>, i32
    %15 = tt.expand_dims %9 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xi32, #mma>
    %16 = tt.expand_dims %10 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<256x1xi32, #blocked>
    %17 = tt.expand_dims %5 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<256x1xi32, #blocked>
    %18 = arith.muli %3, %arg7 : i32
    %19 = tt.splat %arg7 : i32 -> tensor<256x1xi32, #blocked>
    %20 = arith.muli %17, %19 : tensor<256x1xi32, #blocked>
    %21 = tt.addptr %14, %18 : !tt.ptr<f16>, i32
    %22 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>>
    %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>>
    %24 = tt.broadcast %20 : tensor<256x1xi32, #blocked> -> tensor<256x128xi32, #blocked>
    %25 = tt.expand_dims %23 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x128xi32, #blocked>
    %26 = tt.broadcast %25 : tensor<1x128xi32, #blocked> -> tensor<256x128xi32, #blocked>
    %27 = arith.addi %26, %24 : tensor<256x128xi32, #blocked>
    %28 = arith.muli %2, %arg8 : i32
    %29 = tt.addptr %arg1, %28 : !tt.ptr<f16>, i32
    %30 = arith.muli %1, %arg9 : i32
    %31 = tt.addptr %29, %30 : !tt.ptr<f16>, i32
    %32 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>>
    %33 = tt.expand_dims %32 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1>
    %34 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>>
    %35 = tt.broadcast %33 : tensor<128x1xi32, #blocked1> -> tensor<128x64xi32, #blocked1>
    %36 = tt.expand_dims %34 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1>
    %37 = tt.splat %arg10 : i32 -> tensor<1x64xi32, #blocked1>
    %38 = arith.muli %36, %37 : tensor<1x64xi32, #blocked1>
    %39 = tt.broadcast %38 : tensor<1x64xi32, #blocked1> -> tensor<128x64xi32, #blocked1>
    %40 = arith.addi %39, %35 : tensor<128x64xi32, #blocked1>
    %41 = arith.muli %2, %arg11 : i32
    %42 = tt.addptr %arg2, %41 : !tt.ptr<f16>, i32
    %43 = arith.muli %1, %arg12 : i32
    %44 = tt.addptr %42, %43 : !tt.ptr<f16>, i32
    %45 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>>
    %46 = tt.expand_dims %45 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xi32, #blocked>
    %47 = tt.splat %arg13 : i32 -> tensor<64x1xi32, #blocked>
    %48 = arith.muli %46, %47 : tensor<64x1xi32, #blocked>
    %49 = tt.broadcast %48 : tensor<64x1xi32, #blocked> -> tensor<64x128xi32, #blocked>
    %50 = tt.broadcast %25 : tensor<1x128xi32, #blocked> -> tensor<64x128xi32, #blocked>
    %51 = arith.addi %50, %49 : tensor<64x128xi32, #blocked>
    %52 = arith.cmpi slt, %15, %cst_10 : tensor<256x1xi32, #mma>
    %53 = arith.cmpi slt, %16, %cst_5 : tensor<256x1xi32, #blocked>
    %54 = tt.broadcast %52 : tensor<256x1xi1, #mma> -> tensor<256x128xi1, #mma>
    %55 = tt.broadcast %53 : tensor<256x1xi1, #blocked> -> tensor<256x128xi1, #blocked>
    %56 = arith.muli %arg10, %c64_i32 : i32
    %57 = arith.muli %arg13, %c64_i32 : i32
    %58 = arith.addi %0, %c1_i32 : i32
    %59 = arith.muli %58, %c256_i32 : i32
    %60 = arith.muli %2, %c786432_i32 : i32
    %61 = tt.addptr %arg3, %60 : !tt.ptr<f32>, i32
    %62 = arith.muli %1, %c16384_i32 : i32
    %63 = tt.addptr %61, %62 : !tt.ptr<f32>, i32
    %64 = tt.addptr %63, %3 : !tt.ptr<f32>, i32
    %65 = arith.subi %59, %c16384_i32 : i32
    %66 = arith.cmpi sgt, %65, %c0_i32 : i32
    %67 = arith.muli %2, %arg14 : i32
    %68 = tt.addptr %arg4, %67 : !tt.ptr<f16>, i32
    %69 = arith.muli %1, %arg15 : i32
    %70 = tt.addptr %68, %69 : !tt.ptr<f16>, i32
    %71 = tt.expand_dims %4 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xi32, #mma>
    %72 = arith.muli %3, %arg16 : i32
    %73 = tt.splat %arg16 : i32 -> tensor<256x1xi32, #mma>
    %74 = arith.muli %71, %73 : tensor<256x1xi32, #mma>
    %75 = tt.addptr %70, %72 : !tt.ptr<f16>, i32
    %76 = tt.broadcast %74 : tensor<256x1xi32, #mma> -> tensor<256x128xi32, #mma>
    %77 = tt.expand_dims %22 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x128xi32, #mma>
    %78 = tt.broadcast %77 : tensor<1x128xi32, #mma> -> tensor<256x128xi32, #mma>
    %79 = arith.addi %78, %76 : tensor<256x128xi32, #mma>
    %80 = arith.select %66, %54, %cst_9 : tensor<256x128xi1, #mma>
    scf.while (%arg27 = %c0_i32) : (i32) -> () {
      %81 = arith.cmpi slt, %arg27, %c1_i32 : i32
      scf.condition(%81)
    } do {
      %81 = amdgpu.buffer_load %21[%27], %55 stride = %arg7 : tensor<256x128xf16, #blocked>
      %82 = ttg.local_alloc %81 : (tensor<256x128xf16, #blocked>) -> !ttg.memdesc<256x128xf16, #shared, #smem>
      %83 = ttg.local_load %82 : !ttg.memdesc<256x128xf16, #shared, #smem> -> tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>>
      %84 = ttg.local_alloc : () -> !ttg.memdesc<2x128x64xf16, #shared1, #smem, mutable>
      %85 = ttg.local_alloc : () -> !ttg.memdesc<2x64x128xf16, #shared2, #smem, mutable>
      %86 = ttg.memdesc_subview %84[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xf16, #shared1, #smem, mutable>
      %87 = amdgpu.buffer_load_to_local %31[%40] mask = %cst_7 stride = %arg10 into %86 : <f16>[tensor<128x64xi32, #blocked1>]  -> <128x64xf16, #shared1, #smem, mutable>
      %88 = ttg.async_commit_group %87
      %89 = tt.addptr %31, %56 : !tt.ptr<f16>, i32
      %90 = ttg.memdesc_subview %84[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xf16, #shared1, #smem, mutable>
      %91 = amdgpu.buffer_load_to_local %89[%40] mask = %cst_7 stride = %arg10 into %90 : <f16>[tensor<128x64xi32, #blocked1>]  -> <128x64xf16, #shared1, #smem, mutable>
      %92 = ttg.async_commit_group %91
      %93 = ttg.async_wait %88 {num = 2 : i32}
      %94 = ttg.local_load %86 token %93 : !ttg.memdesc<128x64xf16, #shared1, #smem, mutable> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>>
      %95 = ttg.memdesc_subview %85[%c0_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x128xf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>
      %96 = amdgpu.buffer_load_to_local %44[%51] mask = %cst_6 stride = %arg13 into %95 : <f16>[tensor<64x128xi32, #blocked>]  -> <64x128xf16, #shared2, #smem, mutable>
      %97 = ttg.async_commit_group %96
      %98 = tt.dot %83, %94, %cst_8 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x64xf32, #mma>
      %99 = tt.addptr %89, %56 : !tt.ptr<f16>, i32
      %100 = tt.addptr %44, %57 : !tt.ptr<f16>, i32
      %101 = amdgpu.buffer_load_to_local %99[%40] mask = %cst_7 stride = %arg10 into %86 : <f16>[tensor<128x64xi32, #blocked1>]  -> <128x64xf16, #shared1, #smem, mutable>
      %102 = ttg.async_commit_group %101
      %103 = "tt.reduce"(%98) <{axis = 1 : i32}> ({
      ^bb0(%arg27: f32, %arg28: f32):
        %190 = arith.maxnumf %arg27, %arg28 : f32
        tt.reduce.return %190 : f32
      }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %104 = arith.maxnumf %103, %cst_4 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %105 = arith.mulf %104, %cst : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %106 = arith.mulf %98, %cst_0 : tensor<256x64xf32, #mma>
      %107 = tt.expand_dims %105 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
      %108 = tt.broadcast %107 : tensor<256x1xf32, #mma> -> tensor<256x64xf32, #mma>
      %109 = arith.subf %106, %108 : tensor<256x64xf32, #mma>
      %110 = math.exp2 %109 : tensor<256x64xf32, #mma>
      %111 = arith.subf %cst_4, %105 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %112 = math.exp2 %111 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %113 = ttg.async_wait %92 {num = 4 : i32}
      %114 = ttg.local_load %90 token %113 : !ttg.memdesc<128x64xf16, #shared1, #smem, mutable> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>>
      %115 = ttg.memdesc_subview %85[%c1_i32, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x128xf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>
      %116 = amdgpu.buffer_load_to_local %100[%51] mask = %cst_6 stride = %arg13 into %115 : <f16>[tensor<64x128xi32, #blocked>]  -> <64x128xf16, #shared2, #smem, mutable>
      %117 = ttg.async_commit_group %116
      gpu.barrier
      %118 = rocdl.workitem.id.x : i32
      %119 = arith.divsi %118, %c256_i32 : i32
      %120 = arith.cmpi eq, %119, %c0_i32 : i32
      %121 = arith.cmpi ne, %119, %c0_i32 : i32
      amdgpu.cond_barrier %121
      %122:15 = scf.for %arg27 = %c0_i32 to %c8000_i32 step %c64_i32 iter_args(%arg28 = %cst_1, %arg29 = %cst_3, %arg30 = %104, %arg31 = %99, %arg32 = %100, %arg33 = %c0_i32, %arg34 = %114, %arg35 = %110, %arg36 = %112, %arg37 = %97, %arg38 = %117, %arg39 = %95, %arg40 = %115, %arg41 = %102, %arg42 = %86) -> (tensor<256x128xf32, #mma>, tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>, !tt.ptr<f16>, !tt.ptr<f16>, i32, tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>>, tensor<256x64xf32, #mma>, tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>, !ttg.async.token, !ttg.async.token, !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>, !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>, !ttg.async.token, !ttg.memdesc<128x64xf16, #shared1, #smem, mutable>)  : i32 {
        rocdl.s.setprio 0
        %190 = tt.dot %83, %arg34, %cst_8 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x64xf32, #mma>
        %191 = "tt.reduce"(%arg35) <{axis = 1 : i32}> ({
        ^bb0(%arg43: f32, %arg44: f32):
          %226 = arith.addf %arg43, %arg44 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %192 = tt.expand_dims %arg36 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
        %193 = tt.broadcast %192 : tensor<256x1xf32, #mma> -> tensor<256x128xf32, #mma>
        %194 = arith.mulf %arg28, %193 : tensor<256x128xf32, #mma>
        %195 = arith.mulf %arg29, %arg36 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %196 = arith.addf %195, %191 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %197 = arith.truncf %arg35 : tensor<256x64xf32, #mma> to tensor<256x64xf16, #mma>
        rocdl.s.setprio 1
        %198 = ttg.async_wait %arg37 {num = 4 : i32}
        rocdl.sched.barrier 0
        %199 = ttg.local_load %arg39 token %198 : !ttg.memdesc<64x128xf16, #shared2, #smem, mutable> -> tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>
        %200 = tt.addptr %arg31, %56 : !tt.ptr<f16>, i32
        %201 = tt.addptr %arg32, %57 : !tt.ptr<f16>, i32
        %202 = arith.addi %arg33, %c1_i32 : i32
        %203 = arith.cmpi slt, %202, %c2_i32 : i32
        %204 = arith.select %203, %202, %c0_i32 : i32
        %205 = ttg.memdesc_subview %84[%204, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x64xf16, #shared1, #smem, mutable>
        %206 = amdgpu.buffer_load_to_local %200[%40] stride = %arg10 into %205 : <f16>[tensor<128x64xi32, #blocked1>]  -> <128x64xf16, #shared1, #smem, mutable>
        %207 = ttg.async_commit_group %206
        rocdl.sched.barrier 0
        %208 = ttg.convert_layout %197 : tensor<256x64xf16, #mma> -> tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>
        rocdl.s.setprio 0
        %209 = tt.dot %208, %199, %194 : tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<256x128xf32, #mma>
        %210 = "tt.reduce"(%190) <{axis = 1 : i32}> ({
        ^bb0(%arg43: f32, %arg44: f32):
          %226 = arith.maxnumf %arg43, %arg44 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %211 = arith.maxnumf %arg30, %210 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %212 = arith.mulf %211, %cst : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %213 = arith.mulf %190, %cst_0 : tensor<256x64xf32, #mma>
        %214 = tt.expand_dims %212 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
        %215 = tt.broadcast %214 : tensor<256x1xf32, #mma> -> tensor<256x64xf32, #mma>
        %216 = arith.subf %213, %215 : tensor<256x64xf32, #mma>
        %217 = math.exp2 %216 : tensor<256x64xf32, #mma>
        %218 = arith.mulf %arg30, %cst : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %219 = arith.subf %218, %212 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %220 = math.exp2 %219 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        rocdl.s.setprio 1
        %221 = ttg.async_wait %arg41 {num = 4 : i32}
        rocdl.sched.barrier 0
        %222 = ttg.local_load %arg42 token %221 : !ttg.memdesc<128x64xf16, #shared1, #smem, mutable> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>>
        %223 = ttg.memdesc_subview %85[%arg33, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x128xf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>
        %224 = amdgpu.buffer_load_to_local %201[%51] stride = %arg13 into %223 : <f16>[tensor<64x128xi32, #blocked>]  -> <64x128xf16, #shared2, #smem, mutable>
        %225 = ttg.async_commit_group %224
        rocdl.sched.barrier 0
        scf.yield %209, %196, %211, %200, %201, %204, %222, %217, %220, %arg38, %225, %arg40, %223, %207, %205 : tensor<256x128xf32, #mma>, tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>, !tt.ptr<f16>, !tt.ptr<f16>, i32, tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>>, tensor<256x64xf32, #mma>, tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>, !ttg.async.token, !ttg.async.token, !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>, !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>, !ttg.async.token, !ttg.memdesc<128x64xf16, #shared1, #smem, mutable>
      }
      amdgpu.cond_barrier %120
      %123 = tt.dot %83, %122#6, %cst_8 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x64xf32, #mma>
      %124 = "tt.reduce"(%122#7) <{axis = 1 : i32}> ({
      ^bb0(%arg27: f32, %arg28: f32):
        %190 = arith.addf %arg27, %arg28 : f32
        tt.reduce.return %190 : f32
      }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %125 = tt.expand_dims %122#8 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
      %126 = tt.broadcast %125 : tensor<256x1xf32, #mma> -> tensor<256x128xf32, #mma>
      %127 = arith.mulf %122#0, %126 : tensor<256x128xf32, #mma>
      %128 = arith.mulf %122#1, %122#8 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %129 = arith.addf %128, %124 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %130 = arith.truncf %122#7 : tensor<256x64xf32, #mma> to tensor<256x64xf16, #mma>
      %131 = ttg.async_wait %122#9 {num = 0 : i32}
      %132 = ttg.local_load %122#11 token %131 : !ttg.memdesc<64x128xf16, #shared2, #smem, mutable> -> tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>
      %133 = tt.addptr %122#4, %57 : !tt.ptr<f16>, i32
      %134 = "tt.reduce"(%123) <{axis = 1 : i32}> ({
      ^bb0(%arg27: f32, %arg28: f32):
        %190 = arith.maxnumf %arg27, %arg28 : f32
        tt.reduce.return %190 : f32
      }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %135 = arith.maxnumf %122#2, %134 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %136 = arith.mulf %135, %cst : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %137 = arith.mulf %123, %cst_0 : tensor<256x64xf32, #mma>
      %138 = tt.expand_dims %136 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
      %139 = tt.broadcast %138 : tensor<256x1xf32, #mma> -> tensor<256x64xf32, #mma>
      %140 = arith.subf %137, %139 : tensor<256x64xf32, #mma>
      %141 = math.exp2 %140 : tensor<256x64xf32, #mma>
      %142 = arith.mulf %122#2, %cst : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %143 = arith.subf %142, %136 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %144 = math.exp2 %143 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %145 = ttg.convert_layout %130 : tensor<256x64xf16, #mma> -> tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>
      %146 = tt.dot %145, %132, %127 : tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<256x128xf32, #mma>
      %147 = ttg.async_wait %122#13 {num = 0 : i32}
      %148 = ttg.local_load %122#14 token %147 : !ttg.memdesc<128x64xf16, #shared1, #smem, mutable> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>>
      %149 = ttg.memdesc_subview %85[%122#5, %c0_i32, %c0_i32] : !ttg.memdesc<2x64x128xf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x128xf16, #shared2, #smem, mutable>
      %150 = amdgpu.buffer_load_to_local %133[%51] mask = %cst_6 stride = %arg13 into %149 : <f16>[tensor<64x128xi32, #blocked>]  -> <64x128xf16, #shared2, #smem, mutable>
      %151 = ttg.async_commit_group %150
      %152 = tt.dot %83, %148, %cst_8 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<128x64xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x64xf32, #mma>
      %153 = "tt.reduce"(%141) <{axis = 1 : i32}> ({
      ^bb0(%arg27: f32, %arg28: f32):
        %190 = arith.addf %arg27, %arg28 : f32
        tt.reduce.return %190 : f32
      }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %154 = tt.expand_dims %144 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
      %155 = tt.broadcast %154 : tensor<256x1xf32, #mma> -> tensor<256x128xf32, #mma>
      %156 = arith.mulf %146, %155 : tensor<256x128xf32, #mma>
      %157 = arith.mulf %129, %144 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %158 = arith.addf %157, %153 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %159 = arith.truncf %141 : tensor<256x64xf32, #mma> to tensor<256x64xf16, #mma>
      %160 = ttg.async_wait %122#10 {num = 2 : i32}
      %161 = ttg.local_load %122#12 token %160 : !ttg.memdesc<64x128xf16, #shared2, #smem, mutable> -> tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>
      %162 = "tt.reduce"(%152) <{axis = 1 : i32}> ({
      ^bb0(%arg27: f32, %arg28: f32):
        %190 = arith.maxnumf %arg27, %arg28 : f32
        tt.reduce.return %190 : f32
      }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %163 = arith.maxnumf %135, %162 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %164 = arith.mulf %163, %cst : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %165 = arith.mulf %152, %cst_0 : tensor<256x64xf32, #mma>
      %166 = tt.expand_dims %164 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
      %167 = tt.broadcast %166 : tensor<256x1xf32, #mma> -> tensor<256x64xf32, #mma>
      %168 = arith.subf %165, %167 : tensor<256x64xf32, #mma>
      %169 = math.exp2 %168 : tensor<256x64xf32, #mma>
      %170 = arith.subf %136, %164 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %171 = math.exp2 %170 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %172 = ttg.convert_layout %159 : tensor<256x64xf16, #mma> -> tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>
      %173 = tt.dot %172, %161, %156 : tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<256x128xf32, #mma>
      %174 = "tt.reduce"(%169) <{axis = 1 : i32}> ({
      ^bb0(%arg27: f32, %arg28: f32):
        %190 = arith.addf %arg27, %arg28 : f32
        tt.reduce.return %190 : f32
      }) : (tensor<256x64xf32, #mma>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %175 = tt.expand_dims %171 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
      %176 = tt.broadcast %175 : tensor<256x1xf32, #mma> -> tensor<256x128xf32, #mma>
      %177 = arith.mulf %173, %176 : tensor<256x128xf32, #mma>
      %178 = arith.mulf %158, %171 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %179 = arith.addf %178, %174 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
      %180 = arith.truncf %169 : tensor<256x64xf32, #mma> to tensor<256x64xf16, #mma>
      %181 = ttg.async_wait %151 {num = 0 : i32}
      %182 = ttg.local_load %149 token %181 : !ttg.memdesc<64x128xf16, #shared2, #smem, mutable> -> tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>
      %183 = ttg.convert_layout %180 : tensor<256x64xf16, #mma> -> tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>
      %184 = tt.dot %183, %182, %177 : tensor<256x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<256x128xf32, #mma>
      ttg.local_dealloc %84 : !ttg.memdesc<2x128x64xf16, #shared1, #smem, mutable>
      ttg.local_dealloc %85 : !ttg.memdesc<2x64x128xf16, #shared2, #smem, mutable>
      gpu.barrier
      %185 = tt.expand_dims %179 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256x1xf32, #mma>
      %186 = arith.divf %cst_2, %185 : tensor<256x1xf32, #mma>
      %187 = tt.broadcast %186 : tensor<256x1xf32, #mma> -> tensor<256x128xf32, #mma>
      %188 = arith.mulf %184, %187 : tensor<256x128xf32, #mma>
      %189 = arith.truncf %188 : tensor<256x128xf32, #mma> to tensor<256x128xf16, #mma>
      scf.if %66 {
        %190 = arith.subi %c16640_i32, %59 : i32
        %191 = tt.splat %190 : i32 -> tensor<256xi32, #blocked2>
        %192 = arith.cmpi slt, %6, %191 : tensor<256xi32, #blocked2>
        %193 = math.log2 %179 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %194 = arith.addf %163, %193 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %195 = ttg.convert_layout %194 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256xf32, #blocked2>
        amdgpu.buffer_store %195, %64[%6], %192 : tensor<256xf32, #blocked2>
      } else {
        %190 = math.log2 %179 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %191 = arith.addf %163, %190 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>>
        %192 = ttg.convert_layout %191 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<256xf32, #blocked2>
        amdgpu.buffer_store %192, %64[%6] : tensor<256xf32, #blocked2>
      }
      amdgpu.buffer_store %189, %75[%79], %80 stride = %arg16 : tensor<256x128xf16, #mma>
      scf.yield %c1_i32 : i32
    }
    tt.return
  }
}
