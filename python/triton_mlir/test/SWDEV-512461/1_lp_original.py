import numpy as np
from triton_mlir import types as _types
from triton_mlir.extras.context import RAIIMLIRContextModule
from triton_mlir.dialects import tt as ttpp, ttg, scf, llvm, _tt_ops_gen as tt, amdgpu, rocdl
from triton_mlir.dialects.arith import IntegerOverflowFlags
from triton_mlir.ir import ArrayAttr, Type, Attribute
from triton_mlir.extras.dialects.ext import arith
from triton_mlir.extras.dialects import ext
import triton_mlir.extras.dialects.ext.scf
from triton_mlir.extras import types as T

ctx = RAIIMLIRContextModule()

blocked = ttg.BlockedEncodingAttr.get(size_per_thread=[1, 8], threads_per_warp__=[8, 8], warps_per_cta__=[4, 1], order=[1, 0])
blocked1 = ttg.BlockedEncodingAttr.get(size_per_thread=[8, 1], threads_per_warp__=[8, 8], warps_per_cta__=[1, 4], order=[0, 1])
mma = Attribute.parse('#ttg.amd_mfma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>')
shared = ttg.SwizzledSharedEncodingAttr.get(vec=8, per_phase=1, max_phase=8, order=[1, 0])
shared1 = ttg.SwizzledSharedEncodingAttr.get(vec=8, per_phase=1, max_phase=8, order=[0, 1])
smem = ttg.SharedMemorySpaceAttr.get()
dot0 = ttg.DotOperandEncodingAttr.get(op_idx=0, parent=mma, k_width=8)
dot1 = ttg.DotOperandEncodingAttr.get(op_idx=1, parent=mma, k_width=8)
ctx.module.operation.attributes['ttg.num-ctas'] = Attribute.parse('1 : i32')
ctx.module.operation.attributes['ttg.num-warps'] = Attribute.parse('4 : i32')
ctx.module.operation.attributes['ttg.target'] = Attribute.parse('"hip:gfx942"')
ctx.module.operation.attributes['ttg.threads-per-warp'] = Attribute.parse('64 : i32')

@ttpp.jit(arg_attrs=ArrayAttr.parse('[{tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}]'), function_type=T.function(inputs=[ttpp.ptr(T.f16(), 1), ttpp.ptr(T.f16(), 1), ttpp.ptr(T.f16(), 1), T.i32(), T.i32(), T.i32(), T.i32(), T.i32(), T.i32(), T.i32()], results=[]), noinline=False, sym_name='matmul_kernel', sym_visibility='public')
def matmul_kernel(arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9):
    cst = arith.constant(np.full([256, 256], 0.0, np.float32), T.tensor(256, 256, T.f32(), encoding=mma))
    true = arith.constant(True, T.bool())
    c2_i32 = arith.constant(2, T.i32())
    c1_i32 = arith.constant(1, T.i32())
    c63_i32 = arith.constant(63, T.i32())
    c64_i32 = arith.constant(64, T.i32())
    c4_i32 = arith.constant(4, T.i32())
    c256_i32 = arith.constant(256, T.i32())
    c255_i32 = arith.constant(255, T.i32())
    c76_i32 = arith.constant(76, T.i32())
    c8_i32 = arith.constant(8, T.i32())
    c0_i32 = arith.constant(0, T.i32())
    v0 = tt.get_program_id(axis=0)
    v1 = arith.remsi(lhs=v0, rhs=c8_i32)
    v2 = arith.muli(lhs=v1, rhs=c76_i32)
    v3 = arith.divsi(lhs=v0, rhs=c8_i32)
    v4 = arith.addi(lhs=v2, rhs=v3)
    v5 = arith.addi(lhs=arg4, rhs=c255_i32)
    v6 = arith.divsi(lhs=v5, rhs=c256_i32)
    v7 = arith.muli(lhs=v6, rhs=c4_i32)
    v8 = arith.divsi(lhs=v4, rhs=v7)
    v9 = arith.muli(lhs=v8, rhs=c4_i32)
    v10 = arith.remsi(lhs=v4, rhs=v7)
    v11 = arith.addi(lhs=arg3, rhs=c255_i32)
    v12 = arith.divsi(lhs=v11, rhs=c256_i32)
    v13 = arith.subi(lhs=v12, rhs=v9)
    v14 = arith.minsi(lhs=v13, rhs=c4_i32)
    v15 = arith.remsi(lhs=v10, rhs=v14)
    v16 = arith.addi(lhs=v9, rhs=v15)
    v17 = arith.muli(lhs=v16, rhs=c256_i32)
    v18 = tt.make_range(result=T.tensor(256, T.i32(), encoding=ttg.SliceEncodingAttr.get(dim=1, parent=blocked)), start=0, end=256)
    v19 = tt.expand_dims(src=v18, axis=1)
    v20 = arith.muli(lhs=v17, rhs=arg6)
    v21 = tt.splat(result=T.tensor(256, 1, T.i32(), encoding=blocked), src=arg6)
    v22 = arith.muli(lhs=v19, rhs=v21)
    v23 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=arg0, offset=v20)
    v24 = tt.broadcast(result=T.tensor(256, 64, T.i32(), encoding=blocked), src=v22)
    v25 = tt.make_range(result=T.tensor(64, T.i32(), encoding=ttg.SliceEncodingAttr.get(dim=0, parent=blocked)), start=0, end=64)
    v26 = tt.expand_dims(src=v25, axis=0)
    v27 = tt.broadcast(result=T.tensor(256, 64, T.i32(), encoding=blocked), src=v26)
    v28 = arith.addi(lhs=v27, rhs=v24)
    v29 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=v23, offset=c64_i32)
    v30 = arith.addi(lhs=arg5, rhs=c63_i32)
    v31 = arith.divsi(lhs=v30, rhs=c64_i32)
    v32 = arith.cmpi(predicate=4, lhs=v31, rhs=c0_i32)
    v33 = tt.splat(result=T.tensor(256, 64, T.bool(), encoding=blocked), src=v32)
    v34 = amdgpu.buffer_load(result=T.tensor(256, 64, T.f16(), encoding=blocked), ptr=v23, offsets=v28, cache=1, mask=v33)
    v34.owner.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(0)
    v35 = arith.cmpi(predicate=4, lhs=v31, rhs=c1_i32)
    v36 = tt.splat(result=T.tensor(256, 64, T.bool(), encoding=blocked), src=v35)
    v37 = amdgpu.buffer_load(result=T.tensor(256, 64, T.f16(), encoding=blocked), ptr=v29, offsets=v28, cache=1, mask=v36)
    v37.owner.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(0)
    v38 = tt.make_range(result=T.tensor(64, T.i32(), encoding=ttg.SliceEncodingAttr.get(dim=1, parent=blocked1)), start=0, end=64)
    v39 = tt.expand_dims(src=v38, axis=1)
    v40 = tt.broadcast(result=T.tensor(64, 256, T.i32(), encoding=blocked1), src=v39)
    v41 = arith.divsi(lhs=v10, rhs=v14)
    v42 = arith.muli(lhs=v41, rhs=c256_i32)
    v43 = tt.make_range(result=T.tensor(256, T.i32(), encoding=ttg.SliceEncodingAttr.get(dim=0, parent=blocked1)), start=0, end=256)
    v44 = tt.expand_dims(src=v43, axis=0)
    v45 = arith.muli(lhs=v42, rhs=arg7)
    v46 = tt.splat(result=T.tensor(1, 256, T.i32(), encoding=blocked1), src=arg7)
    v47 = arith.muli(lhs=v44, rhs=v46)
    v48 = tt.broadcast(result=T.tensor(64, 256, T.i32(), encoding=blocked1), src=v47)
    v49 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=arg1, offset=v45)
    v50 = arith.addi(lhs=v48, rhs=v40)
    v51 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=v49, offset=c64_i32)
    v52 = tt.splat(result=T.tensor(64, 256, T.bool(), encoding=blocked1), src=v32)
    v53 = amdgpu.buffer_load(result=T.tensor(64, 256, T.f16(), encoding=blocked1), ptr=v49, offsets=v50, cache=1, mask=v52)
    v53.owner.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(1)
    v54 = tt.splat(result=T.tensor(64, 256, T.bool(), encoding=blocked1), src=v35)
    v55 = amdgpu.buffer_load(result=T.tensor(64, 256, T.f16(), encoding=blocked1), ptr=v51, offsets=v50, cache=1, mask=v54)
    v55.owner.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(1)
    v56 = arith.cmpi(predicate=4, lhs=arg6, rhs=c0_i32)
    llvm.intr_assume(cond=v56, op_bundle_operands=[], op_bundle_sizes=[])
    llvm.intr_assume(cond=true, op_bundle_operands=[], op_bundle_sizes=[])
    llvm.intr_assume(cond=true, op_bundle_operands=[], op_bundle_sizes=[])
    v57 = arith.cmpi(predicate=4, lhs=arg7, rhs=c0_i32)
    llvm.intr_assume(cond=v57, op_bundle_operands=[], op_bundle_sizes=[])
    v58 = arith.cmpi(predicate=4, lhs=arg8, rhs=c0_i32)
    llvm.intr_assume(cond=v58, op_bundle_operands=[], op_bundle_sizes=[])
    llvm.intr_assume(cond=true, op_bundle_operands=[], op_bundle_sizes=[])
    v59 = arith.cmpi(predicate=4, lhs=arg9, rhs=c0_i32)
    llvm.intr_assume(cond=v59, op_bundle_operands=[], op_bundle_sizes=[])
    v60 = arith.cmpi(predicate=4, lhs=v16, rhs=c0_i32)
    llvm.intr_assume(cond=v60, op_bundle_operands=[], op_bundle_sizes=[])
    v61 = arith.cmpi(predicate=4, lhs=v41, rhs=c0_i32)
    llvm.intr_assume(cond=v61, op_bundle_operands=[], op_bundle_sizes=[])
    v62 = tt.make_range(result=T.tensor(256, T.i32(), encoding=ttg.SliceEncodingAttr.get(dim=1, parent=mma)), start=0, end=256)
    v63 = tt.make_range(result=T.tensor(256, T.i32(), encoding=ttg.SliceEncodingAttr.get(dim=0, parent=mma)), start=0, end=256)
    v66 = ttg.local_alloc(result=ttg.MemDescType.get(shape=[1, 256, 64], element_type=T.f16(), encoding=shared, memory_space=smem, mutable_memory=True, alloc_shape=[1, 256, 64]))
    v67 = ttg.local_alloc(result=ttg.MemDescType.get(shape=[1, 64, 256], element_type=T.f16(), encoding=shared1, memory_space=smem, mutable_memory=True, alloc_shape=[1, 64, 256]))
    v68 = ttg.memdesc_subview(result=ttg.MemDescType.get(shape=[256, 64], element_type=T.f16(), encoding=shared, memory_space=smem, mutable_memory=True, alloc_shape=[256, 64]), src=v66, offsets=[c0_i32, c0_i32, c0_i32])
    ttg.local_store_4 = ttg.local_store(src=v34, dst=v68)
    ttg.local_store_4.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(0)
    v69 = ttg.memdesc_subview(result=ttg.MemDescType.get(shape=[64, 256], element_type=T.f16(), encoding=shared1, memory_space=smem, mutable_memory=True, alloc_shape=[64, 256]), src=v67, offsets=[c0_i32, c0_i32, c0_i32])
    ttg.local_store_5 = ttg.local_store(src=v53, dst=v69)
    ttg.local_store_5.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(1)
    v70 = ttg.local_load(result=T.tensor(256, 64, T.f16(), encoding=dot0), src=v68)
    v71 = ttg.local_load(result=T.tensor(64, 256, T.f16(), encoding=dot1), src=v69)
    v72 = arith.subi(lhs=v31, rhs=c2_i32)
    for arg10, [arg11, arg12, arg13, arg14, arg15, arg16, arg17, arg18], [v73_0, v73_1, v73_2, v73_3, v73_4, v73_5, v73_6, v73_7] in scf.for_(c0_i32, v72, c1_i32, iter_args=[cst, c0_i32, v37, v55, v70, v71, v29, v51]):
        v96 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=arg17, offset=c64_i32)
        v97 = amdgpu.buffer_load(result=T.tensor(256, 64, T.f16(), encoding=blocked), ptr=v96, offsets=v28, cache=1)
        v97.owner.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(0)
        v98 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=arg18, offset=c64_i32)
        v99 = arith.addi(lhs=arg12, rhs=c1_i32)
        v100 = arith.cmpi(predicate=2, lhs=v99, rhs=c1_i32)
        v101 = arith.select(condition=v100, true_value=v99, false_value=c0_i32)
        v102 = ttg.memdesc_subview(result=ttg.MemDescType.get(shape=[256, 64], element_type=T.f16(), encoding=shared, memory_space=smem, mutable_memory=True, alloc_shape=[256, 64]), src=v66, offsets=[v101, c0_i32, c0_i32])
        ttg.local_store_7 = ttg.local_store(src=arg13, dst=v102)
        ttg.local_store_7.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(0)
        v103 = ttg.memdesc_subview(result=ttg.MemDescType.get(shape=[64, 256], element_type=T.f16(), encoding=shared1, memory_space=smem, mutable_memory=True, alloc_shape=[64, 256]), src=v67, offsets=[v101, c0_i32, c0_i32])
        ttg.local_store_8 = ttg.local_store(src=arg14, dst=v103)
        ttg.local_store_8.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(1)
        v104 = amdgpu.buffer_load(result=T.tensor(64, 256, T.f16(), encoding=blocked1), ptr=v98, offsets=v50, cache=1)
        v104.owner.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(1)
        v105 = tt.dot(a=arg15, b=arg16, c=arg11, input_precision=2, max_num_imprecise_acc=0)
        v106 = ttg.local_load(result=T.tensor(256, 64, T.f16(), encoding=dot0), src=v102)
        v107 = ttg.local_load(result=T.tensor(64, 256, T.f16(), encoding=dot1), src=v103)
        scf.yield_(results_=[v105, v101, v97, v104, v106, v107, v96, v98])
    v74 = arith.cmpi(predicate=5, lhs=v31, rhs=c1_i32)
    v75 = arith.cmpi(predicate=5, lhs=v31, rhs=c2_i32)
    v76 = arith.addi(lhs=v73_1, rhs=c1_i32)
    v77 = arith.cmpi(predicate=2, lhs=v76, rhs=c1_i32)
    v78 = arith.select(condition=v77, true_value=v76, false_value=c0_i32)
    v79 = ttg.memdesc_subview(result=ttg.MemDescType.get(shape=[256, 64], element_type=T.f16(), encoding=shared, memory_space=smem, mutable_memory=True, alloc_shape=[256, 64]), src=v66, offsets=[v78, c0_i32, c0_i32])
    ttg.local_store_10 = ttg.local_store(src=v73_2, dst=v79)
    ttg.local_store_10.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(0)
    v80 = ttg.memdesc_subview(result=ttg.MemDescType.get(shape=[64, 256], element_type=T.f16(), encoding=shared1, memory_space=smem, mutable_memory=True, alloc_shape=[64, 256]), src=v67, offsets=[v78, c0_i32, c0_i32])
    ttg.local_store_11 = ttg.local_store(src=v73_3, dst=v80)
    ttg.local_store_11.attributes['OpIdx'] = amdgpu.OpIdxAttr.get(1)
    @ext.scf.if_(v74, results=[T.tensor(256, 256, T.f32(), encoding=mma)])
    def v81():                
        v96 = tt.dot(a=v73_4, b=v73_5, c=v73_0, input_precision=2, max_num_imprecise_acc=0)
        return v96
    @ext.scf.else_(v81)
    def v81_else():                
        return v73_0
    v82 = ttg.local_load(result=T.tensor(256, 64, T.f16(), encoding=dot0), src=v79)
    v83 = ttg.local_load(result=T.tensor(64, 256, T.f16(), encoding=dot1), src=v80)
    v84 = arith.select(condition=v74, true_value=v81, false_value=v73_0)
    @ext.scf.if_(v75, results=[T.tensor(256, 256, T.f32(), encoding=mma)])
    def v85():                
        v96 = tt.dot(a=v82, b=v83, c=v84, input_precision=2, max_num_imprecise_acc=0)
        return v96
    @ext.scf.else_(v85)
    def v85_else():                
        return v84
    v86 = arith.select(condition=v75, true_value=v85, false_value=v84)
    ttg.local_dealloc(src=v66)
    ttg.local_dealloc(src=v67)
    v87 = arith.truncf(out=T.tensor(256, 256, T.f16(), encoding=mma), in_=v86)
    v88 = arith.muli(lhs=arg8, rhs=v17)
    v89 = tt.splat(result=T.tensor(256, 1, T.i32(), encoding=mma), src=arg8)
    v64 = tt.expand_dims(src=v62, axis=1)
    v90 = arith.muli(lhs=v89, rhs=v64)
    v91 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=arg2, offset=v88)
    v92 = tt.broadcast(result=T.tensor(256, 256, T.i32(), encoding=mma), src=v90)
    v65 = tt.expand_dims(src=v63, axis=0)
    v93 = tt.broadcast(result=T.tensor(256, 256, T.i32(), encoding=mma), src=v65)
    v94 = tt.addptr(result=ttpp.ptr(T.f16(), 1), ptr=v91, offset=v42)
    v95 = arith.addi(lhs=v93, rhs=v92)
    amdgpu.buffer_store(value=v87, ptr=v94, offsets=v95, cache=1)

matmul_kernel.emit()
ctx.module.operation.verify()

def mod_str():
    return str(ctx.module)

